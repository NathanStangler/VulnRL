# metrics_dashboard.ipynb
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, classification_report

# Load test results and compiler feedback logs
results = pd.read_json("feedback_logs.jsonl", lines=True)

# Plot reward distribution
plt.figure(figsize=(6,4))
sns.histplot(results["reward"], bins=15, kde=True)
plt.title("Reward Distribution from Compiler Feedback")
plt.xlabel("Reward Score")
plt.ylabel("Frequency")
plt.show()

# (Optional) Confusion matrix example if predictions available
y_true = results["true_label"] if "true_label" in results else []
y_pred = results["pred_label"] if "pred_label" in results else []

if y_true and y_pred:
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Safe","Vulnerable"], yticklabels=["Safe","Vulnerable"])
    plt.title("Confusion Matrix")
    plt.show()

# Summary
print("Compiler feedback summary:")
print(results.describe(include="all"))
