{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e1a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from code_chunker import build_chunks\n",
    "from data_processing import get_dataset, tokenize_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import tqdm\n",
    "from collections import Counter\n",
    "import tempfile\n",
    "import sklearn.metrics\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71db4145",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('qwen25_coder_1_5b_instruct')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = AutoModelForCausalLM.from_pretrained('qwen25_coder_1_5b_instruct')\n",
    "\n",
    "_, _, test = get_dataset()\n",
    "test_dataset = tokenize_dataset(test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a37be83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(prompt, tokenizer, model):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"Analyze the following C++ code and classify its vulnerability. Your classification should be one of the following: Improper Input Validation, Improper Limitation of a Pathname to a Restricted Directory (“Path Traversal”), Improper Neutralization of Special Elements used in an OS Command (“OS Command Injection”), Improper Neutralization of Input During Web Page Generation (“Cross-site Scripting”), Improper Neutralization of Special Elements used in an SQL Command (“SQL Injection”), Improper Control of Generation of Code (“Code Injection”), Improper Output Neutralization for Logs, Integer Overflow or Wraparound, NULL Pointer Dereference, Deserialization of Untrusted Data, URL Redirection to Untrusted Site (“Open Redirect”), Improper Restriction of XML External Entity Reference, Out-of-bounds Write, safe. Only respond with the classification.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=1024\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2f50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for sample in tqdm.tqdm(test_dataset):\n",
    "    prompt = sample[\"code\"]\n",
    "    label = sample[\"output\"].strip().lower()\n",
    "    prediction = get_prediction(prompt, tokenizer, model).strip().lower()\n",
    "    y_true.append(label)\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "precision = sklearn.metrics.precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "recall = sklearn.metrics.recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "f1 = sklearn.metrics.f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "def to_binary(label):\n",
    "    return \"safe\" if label == \"safe\" else \"unsafe\"\n",
    "\n",
    "y_true_binary = [to_binary(label) for label in y_true]\n",
    "y_pred_binary = [to_binary(pred) for pred in y_pred]\n",
    "\n",
    "binary_accuracy = sklearn.metrics.accuracy_score(y_true_binary, y_pred_binary)\n",
    "binary_precision = sklearn.metrics.precision_score(y_true_binary, y_pred_binary, pos_label=\"safe\", zero_division=0)\n",
    "binary_recall = sklearn.metrics.recall_score(y_true_binary, y_pred_binary, pos_label=\"safe\", zero_division=0)\n",
    "binary_f1 = sklearn.metrics.f1_score(y_true_binary, y_pred_binary, pos_label=\"safe\", zero_division=0)\n",
    "\n",
    "print(f\"Binary Accuracy (safe/unsafe): {binary_accuracy:.4f}\")\n",
    "print(f\"Binary Precision (safe): {binary_precision:.4f}\")\n",
    "print(f\"Binary Recall (safe): {binary_recall:.4f}\")\n",
    "print(f\"Binary F1 Score (safe): {binary_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_code(code, tokenizer, model):\n",
    "    with tempfile.TemporaryDirectory() as directory:\n",
    "        with open(os.path.join(directory, \"main.cpp\"), \"w\") as f:\n",
    "            f.write(code)\n",
    "        chunks = build_chunks(directory, tokenizer.encode, max_tokens=1024, overlap=128)\n",
    "        responses = [get_prediction(chunk[\"code\"], tokenizer, model).strip().lower() for chunk in chunks]\n",
    "        return Counter(responses).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3091a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "for sample in tqdm.tqdm(test_dataset):\n",
    "    code = sample[\"code\"]\n",
    "    label = sample[\"output\"].strip().lower()\n",
    "    prediction = predict_single_code(code, tokenizer, model)\n",
    "    y_true.append(label)\n",
    "    y_pred.append(prediction)\n",
    "\n",
    "accuracy = sklearn.metrics.accuracy_score(y_true, y_pred)\n",
    "precision = sklearn.metrics.precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "recall = sklearn.metrics.recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "f1 = sklearn.metrics.f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)\n",
    "\n",
    "print(f\"Chunked Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Chunked Precision: {precision:.4f}\")\n",
    "print(f\"Chunked Recall: {recall:.4f}\")\n",
    "print(f\"Chunked F1 Score: {f1:.4f}\")\n",
    "\n",
    "def to_binary(label):\n",
    "    return \"safe\" if label == \"safe\" else \"unsafe\"\n",
    "\n",
    "y_true_binary = [to_binary(label) for label in y_true]\n",
    "y_pred_binary = [to_binary(pred) for pred in y_pred]\n",
    "\n",
    "binary_accuracy = sklearn.metrics.accuracy_score(y_true_binary, y_pred_binary)\n",
    "binary_precision = sklearn.metrics.precision_score(y_true_binary, y_pred_binary, pos_label=\"safe\", zero_division=0)\n",
    "binary_recall = sklearn.metrics.recall_score(y_true_binary, y_pred_binary, pos_label=\"safe\", zero_division=0)\n",
    "binary_f1 = sklearn.metrics.f1_score(y_true_binary, y_pred_binary, pos_label=\"safe\", zero_division=0)\n",
    "\n",
    "print(f\"Chunked Binary Accuracy (safe/unsafe): {binary_accuracy:.4f}\")\n",
    "print(f\"Chunked Binary Precision (safe): {binary_precision:.4f}\")\n",
    "print(f\"Chunked Binary Recall (safe): {binary_recall:.4f}\")\n",
    "print(f\"Chunked Binary F1 Score (safe): {binary_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
